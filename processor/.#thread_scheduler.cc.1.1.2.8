/* Copyright (c) 2005 by Gurindar S. Sohi for the Wisconsin
 * Multiscalar Project.  ALL RIGHTS RESERVED.
 *
 * This software is furnished under the Multiscalar license.
 * For details see the LICENSE.mscalar file in the top-level source
 * directory, or online at http://www.cs.wisc.edu/mscalar/LICENSE
 *
 */

/* description:    thread scheduler for mapping threads to sequencers
 * initial author: Philip Wells 
 *
 */
 
#include "simics/first.h"
RCSID("$Id: thread_scheduler.cc,v 1.1.2.8 2005/11/04 21:58:09 kchak Exp $");

#include "definitions.h"
#include "isa.h"
#include "dynamic.h"
#include "proc_stats.h"
#include "chip.h"
#include "stats.h"
#include "counter.h"
#include "mai_instr.h"
#include "sequencer.h"
#include "verbose_level.h"
#include "config_extern.h"
#include "histogram.h"
#include "mai.h"
#include "sequencer.h"

#include "thread_scheduler.h"

hw_context_t::hw_context_t (sequencer_t *_s, uint32 _ct, bool _b)
    : seq(_s), ctxt(_ct), busy(_b), curr_sys_call(0)
{
    
}
        

thread_scheduler_t::thread_scheduler_t(chip_t *_chip)
{
	chip = _chip;
	
	num_user = g_conf_num_user_seq;
	num_hw_ctxt = 0;
    
	num_threads = chip->get_num_cpus();
    preferred_user_ctxt = new uint32[num_threads];
    next_hw_context_assignment = g_conf_num_user_seq;
    
    
    if (g_conf_chip_design[0]) {
        uint32 count = 0;
        num_seq = g_conf_chip_design[0];
        for (uint32 i = 1; i <= num_seq; i++)
            num_hw_ctxt += g_conf_chip_design[i];
        printf("Number of hardware contexts = %d\n", num_hw_ctxt);
        hw_context = new (hw_context_t *)[num_hw_ctxt];
        
        for (uint32 i = 1 ; i <= num_seq; i++)
        {
            for (uint32 j = 0; j < g_conf_chip_design[i]; j++, count++)
                hw_context[count] = new hw_context_t(chip->get_sequencer(i - 1), 
                    j, (count < num_threads));
        }
        
        // Provision extra h/w contexts for OS
        for (uint32 i = num_threads; i < num_hw_ctxt; i++)
            idle_os_ctxt.push_back(hw_context[i]);
        
        
    } else {
        num_hw_ctxt = num_threads;
        hw_context = new hw_context_t *[num_hw_ctxt];
        for (uint32 i = 0; i < num_hw_ctxt; i++)
            hw_context[i] = new hw_context_t (chip->get_sequencer(i), 0, true);
    }
    
    if (g_conf_syscall_provision[0]) provision_syscall();
    
    // Set  up the user ctxt
    if (g_conf_num_user_seq) {
        uint32 curr_ctxt = 0;
        for (uint32 i = 0; i < num_threads; i++)
        {
            preferred_user_ctxt[i] = curr_ctxt++;
            if (curr_ctxt == g_conf_num_user_seq) curr_ctxt = 0;
        }
    }
}


void
thread_scheduler_t::provision_syscall()
{
    ASSERT(g_conf_num_user_seq);
    uint32 count = g_conf_syscall_provision[0];
    uint32 syscall, requirements;
    for (uint32 i = 0; i < count; i++)
    {
        syscall = g_conf_syscall_provision[ i * 2 + 1];
        requirements = g_conf_syscall_provision [ i * 2 + 2];
        printf("Assigning syscall %d to %d contexts\n", syscall, requirements);
        assign_new_syscall_ctxt(syscall, requirements);
    }
    
}

void
thread_scheduler_t::assign_new_syscall_ctxt(uint32 syscall, uint32 req)
{
    for (uint32 j = 0; j < req; j++)
    {
        syscall_context[syscall].insert(hw_context[next_hw_context_assignment++]);
        if (next_hw_context_assignment == num_hw_ctxt) 
            next_hw_context_assignment = g_conf_num_user_seq;
    }
}


bool
thread_scheduler_t::thread_yield(sequencer_t *seq, uint32 ctxt, mai_t *mai,
	ts_yield_reason_t why)
{
	// A currently running sequencer thinks its thread may need to switch
	// Check for sure if this is what we want to do and do it.

	if (!g_conf_separate_user_os) 
		return false;

	bool user_seq = is_user_ctxt(seq, ctxt); 

	switch(why) {
		case YIELD_LONG_RUNNING:
			if (user_seq && !wait_for_user.empty())
				return true;
			if (!user_seq && !wait_for_os.empty())
				return true;
			break;
	
		case YIELD_MUTEX_LOCKED:
			ASSERT(mai->is_supervisor());
			if (!wait_for_os.empty())
				return true;
			break;
	
		case YIELD_DONE_RETRY:
			// returned to user
			if (!mai->is_supervisor() && !user_seq)
				return true;
			break;

		case YIELD_EXCEPTION:
			ASSERT(mai->get_tl() > 0);
			ASSERT(mai->is_supervisor());
			if (is_syscall_trap(mai->get_tt(mai->get_tl())))
            {
                ASSERT(mai->get_tl() == 1);
                mai->set_syscall_num();
				return true;
            }
			break;
			
		default:
			FAIL;
			return false;
	}

	return false;
}


hw_context_t *thread_scheduler_t::match_user_context(mai_t *mai)
{
    hw_context_t *ret = 0;
    // Policy :-
    // Choose (if possible) the context the thread used last time
        
    uint32 preferred_context = preferred_user_ctxt[mai->get_id()];
    if (hw_context[preferred_context]->busy == false)
    {
        ret = hw_context[preferred_context];   
    }
    else if (idle_user_ctxt.size())
    {
        ret = idle_user_ctxt.front();
        idle_user_ctxt.pop_front();
    }
    
    return ret;
}

void thread_scheduler_t::debug_idle_context(list<hw_context_t *> l)
{
    list<hw_context_t *>::iterator it = l.begin();
    while (it != l.end())
    {
        hw_context_t *hwc = *it;
        DEBUG_OUT("seq = %d ctxt = %d\n", hwc->seq->get_id(), hwc->ctxt);
        it++;
    }
}

hw_context_t *thread_scheduler_t::get_syscall_context(uint32 syscall)
{
    
    hw_context_t *ret = 0;
    map<uint32, set<hw_context_t *> >::iterator it = syscall_context.find(syscall);
    if (it != syscall_context.end())
    {
        set<hw_context_t *> ctxt_set = it->second;
        set<hw_context_t *>::iterator c_it = ctxt_set.begin();
        while (c_it != ctxt_set.end())
        {
            hw_context_t *desired = *c_it;
            if (!desired->busy)
            {
                //debug_idle_context(idle_os_ctxt);
                ASSERT(find(idle_os_ctxt.begin(), idle_os_ctxt.end(), desired)
                    != idle_os_ctxt.end());
                ret = desired;
                idle_os_ctxt.remove(desired);
                break;
            }
            c_it++;
        }
            
        ASSERT(idle_os_ctxt.size() < (num_hw_ctxt - g_conf_num_user_seq)); 
        
    } else {
        // First time occurence map to the most easily available
        if (!idle_os_ctxt.empty()) {
            ret = idle_os_ctxt.front();
            syscall_context[syscall].insert(ret);
            idle_os_ctxt.pop_front();
        } else {
            assign_new_syscall_ctxt(syscall, 1);
        }
    }
    
    return ret;
    
}
    
hw_context_t *thread_scheduler_t::maximize_cache_ctxt(mai_t *mai, bool desire_user_ctxt,  
    uint32 syscall)
{
    hw_context_t *ret = 0;
    if (!desire_user_ctxt)
    {
        ret = get_syscall_context(syscall);   
    } else {
        hw_context_t *preferred = hw_context[preferred_user_ctxt[mai->get_id()]];
        if (!preferred->busy) 
        {
            ret = preferred;
            idle_user_ctxt.remove(preferred);
        }
    }
    
    return ret;
}

mai_t *thread_scheduler_t::maximize_cache_thread(hw_context_t *hwc, bool user_ctxt)
{
    mai_t *ret = 0;
    if (user_ctxt && !wait_for_user.empty())
    {
        list<mai_t *>::iterator it = wait_for_user.begin();
        while (it != wait_for_user.end())
        {
            mai_t *candidate = *it;
            if (hwc == hw_context [preferred_user_ctxt [candidate->get_id()] ])
            {
                ret = candidate;
                wait_for_user.erase(it);
                break;
            }
            it++;
        }
    } else if (!user_ctxt && !wait_for_os.empty()) {
        list<mai_t *>::iterator it = wait_for_os.begin();
        while (it != wait_for_os.end())
        {
            mai_t *candidate = *it;
            uint32 syscall = candidate->get_syscall_num();
            map<uint32, set<hw_context_t *> >::iterator syscall_it = syscall_context.find(syscall);
            ASSERT(syscall_it != syscall_context.end());
            if (syscall_it != syscall_context.end() &&
                syscall_it->second.find(hwc) != syscall_it->second.end())
            {
                ret = candidate;
                wait_for_os.erase(it);
                break;
            }
            it++;
        }   
    }
    
    return ret;
}

hw_context_t *thread_scheduler_t::eager_fifo_ctxt(mai_t *mai, bool desire_user_ctxt, uint32 syscall)
{
    // Eager scheduling: return h.w context in FIFO manner
    
    hw_context_t *ret = 0;
    if (!desire_user_ctxt && !idle_os_ctxt.empty())
    {
        ret = idle_os_ctxt.front();
        idle_os_ctxt.pop_front();
    } else if (desire_user_ctxt && !idle_user_ctxt.empty()) {
        ret = idle_user_ctxt.front();
        idle_user_ctxt.pop_front();
    }
    
    return ret;
    
}
        
mai_t *thread_scheduler_t::eager_fifo_thread(hw_context_t *hwc, bool user_ctxt)
{
    mai_t *ret = 0;
    if (user_ctxt && !wait_for_user.empty()) {
		ret = wait_for_user.front();
		wait_for_user.pop_front();

	} else if (!user_ctxt && !wait_for_os.empty()) {
		ret = wait_for_os.front();
		wait_for_os.pop_front();
	}
    
    return ret;
    
}

hw_context_t *thread_scheduler_t::find_ctxt_for_thread(mai_t *mai, bool desire_user_ctxt,
    uint32 syscall)
{
    
    hw_context_t *ret = NULL;
	
    switch (g_conf_scheduling_algorithm) {
        case 0:
            ret = eager_fifo_ctxt(mai, desire_user_ctxt, syscall);
            break;
        case 1:
            ret = maximize_cache_ctxt(mai, desire_user_ctxt, syscall);
            break;
        default:
            FAIL_MSG("Invalid Scheduling Specified");
    }
            
    // Update current syscall info
    if (ret && !desire_user_ctxt) ret->curr_sys_call = syscall;
    
    return ret;
}

mai_t *thread_scheduler_t::find_thread_for_ctxt(hw_context_t *hwc, bool user_ctxt)
{
    mai_t *ret = 0;
    
    switch (g_conf_scheduling_algorithm) {
        case 0:
            ret = eager_fifo_thread(hwc, user_ctxt);
            break;
        case 1:
            ret = maximize_cache_thread(hwc, user_ctxt);
            break;
        default:
            FAIL_MSG("Invalid Scheduling Specified");
    }
    
    return ret;
    
}

void
thread_scheduler_t::ready_for_switch(sequencer_t *seq, uint32 ctxt, mai_t *mai,
	ts_yield_reason_t why)
{
	
	// Something may have changed, and we now don't want to switch
	if (!thread_yield(seq, ctxt, mai, why))
		return;
	
	bool user_ctxt = is_user_ctxt(seq, ctxt); 
	bool desire_user_seq = !mai->is_supervisor();  // may == user_seq if preemted

    FE_EXCEPTION_CHECK;
	
	// find idle seq, or put thread on idle list
    
    uint32 syscall = 0;
    if (!desire_user_seq && why == YIELD_EXCEPTION) syscall = mai->get_syscall_num();
    
    //DEBUG_OUT("%8llu: Switching thread %d From seq %d ctxt %d need %u\n",  chip->get_g_cycles(), 
    //    mai->get_id(), seq->get_id(), ctxt, desire_user_seq);
    
    hw_context_t *hw_ctxt = search_hw_context(seq, ctxt);
    
    hw_context_t *new_ctxt = find_ctxt_for_thread(mai, desire_user_seq, syscall);
    
	if (new_ctxt) {
        //DEBUG_OUT("%8llu: Selecting seq %d ctxt %d for thread id %d\n", 
        //     chip->get_g_cycles(), new_ctxt->seq->get_id(), new_ctxt->ctxt, mai->get_id());
            
		chip->switch_to_thread(new_ctxt->seq, new_ctxt->ctxt, mai);
        new_ctxt->busy = true;
        //debug_idle_context(idle_os_ctxt);
	} else {
		if (desire_user_seq)
			wait_for_user.push_back(mai);
		else
			wait_for_os.push_back(mai);
        chip->idle_thread_context(mai->get_id());
	}

	// check for matching idle thread to run on newly opened seq	
	// or put old seq on idle list
	mai_t *new_thread = find_thread_for_ctxt(hw_ctxt, user_ctxt);
    
	if (!new_thread) {
        //DEBUG_OUT("%8llu : Idling seq %d ctxt %d \n", chip->get_g_cycles(), 
        //    hw_ctxt->seq->get_id(), hw_ctxt->ctxt);
        hw_ctxt->busy = false;
		if (user_ctxt)
			idle_user_ctxt.push_back(hw_ctxt);
		else {
			idle_os_ctxt.push_back(hw_ctxt);
        //    debug_idle_context(idle_os_ctxt);
        }
	} else {
        //DEBUG_OUT("%8llu Selecting seq %d ctxt %d for thread id %d\n", 
        //    chip->get_g_cycles(), hw_ctxt->seq->get_id(), hw_ctxt->ctxt, new_thread->get_id());
        hw_ctxt->busy = true;    
    }

    //DEBUG_OUT("%8llu: IDLE OS ctxt %d IDLE USER CTXT %d IDLE OS THREAD %d IDLE USER THREAD %d\n",
    //    chip->get_g_cycles(), idle_os_ctxt.size(), idle_user_ctxt.size(), wait_for_os.size(),
    //        wait_for_user.size());
	chip->switch_to_thread(seq, ctxt, new_thread);
			
}


void 
thread_scheduler_t::write_checkpoint(FILE *file)
{
    // Put the OS contexts first
    list<hw_context_t *>::iterator it;
    hw_context_t *hwc;
    
    fprintf(file, "%u\n", idle_os_ctxt.size());
    for (it = idle_os_ctxt.begin(); it != idle_os_ctxt.end(); it++)
    {
        hwc = *it;
        ASSERT(hwc->seq->get_mai_object(hwc->ctxt) == 0);
        fprintf(file, "%u %u\n", hwc->seq->get_id(), hwc->ctxt);
    }
    
    fprintf(file, "%u\n", idle_user_ctxt.size());
    for (it = idle_user_ctxt.begin(); it != idle_user_ctxt.end(); it++)
    {
        hwc = *it;
        ASSERT(hwc->seq->get_mai_object(hwc->ctxt) == 0);
        fprintf(file, "%u %u\n", hwc->seq->get_id(), hwc->ctxt);
    }
    
    // Now the idle cpus
    list<mai_t *>::iterator t_it;
    fprintf(file, "%u\n", wait_for_os.size());
    for (t_it = wait_for_os.begin(); t_it != wait_for_os.end(); t_it++)
    {
        mai_t *thread = *t_it;
        fprintf(file, "%u\n", thread->get_id());
    }
    
    fprintf(file, "%u\n", wait_for_user.size());
    for (t_it = wait_for_user.begin(); t_it != wait_for_user.end(); t_it++)
    {
        mai_t *thread = *t_it;
        fprintf(file, "%u\n", thread->get_id());
    }
    
}

void thread_scheduler_t::read_checkpoint(FILE *file)
{
    
    // clear out the idle context list
    idle_os_ctxt.clear();
    idle_user_ctxt.clear();
    
    list<hw_context_t *>::iterator it;
    hw_context_t *hwc;
 
    uint32 idle_ctxt, seq_id, _ct;
    fscanf(file, "%u\n", &idle_ctxt);
    for (uint32 i = 0; i < idle_ctxt; i++)
    {
        fscanf(file, "%u %u\n", &seq_id, &_ct);
        hwc = search_hw_context(chip->get_sequencer(seq_id), _ct);
        ASSERT(chip->get_sequencer(seq_id)->get_mai_object(_ct) == NULL);
        idle_os_ctxt.push_back(hwc);
    }
    
    fscanf(file, "%u\n", &idle_ctxt);
    for (uint32 i = 0; i < idle_ctxt; i++)
    {
        fscanf(file, "%u %u\n", &seq_id, &_ct);
        ASSERT(chip->get_sequencer(seq_id)->get_mai_object(_ct) == NULL);
        hwc = search_hw_context(chip->get_sequencer(seq_id), _ct);
        idle_user_ctxt.push_back(hwc);
    }
    
    uint32 thread_id, idle_threads;
    fscanf(file, "%u\n", &idle_threads);
    for (uint32 i = 0; i < idle_threads; i++)
    {
        fscanf(file , "%u\n", &thread_id);
        mai_t *thread = chip->get_mai_object(thread_id);
        wait_for_os.push_back(thread);
    }
    
    fscanf(file, "%u\n", &idle_threads);
    for (uint32 i = 0; i < idle_threads; i++)
    {
        fscanf(file , "%u\n", &thread_id);
        mai_t *thread = chip->get_mai_object(thread_id);
        wait_for_user.push_back(thread);
    }
    
}

bool
thread_scheduler_t::is_user_ctxt(sequencer_t *_s, uint32 _c) {
    for (uint32 i = 0; i < num_hw_ctxt; i++)
        if (hw_context[i]->seq == _s && hw_context[i]->ctxt == _c)
            return (i < num_user);
    FAIL;
}

bool
thread_scheduler_t::is_os_ctxt(sequencer_t *_s, uint32 _c) {
	for (uint32 i = 0; i < num_hw_ctxt; i++)
        if (hw_context[i]->seq == _s && hw_context[i]->ctxt == _c)
            return (i >= num_user);
    FAIL;
}

bool
thread_scheduler_t::is_syscall_trap(uint64 tt)
{
	return (
		               //   LINUX TT                        OPEN SOLARIS TT DOC
		tt == 0x100 || // SunOS Syscall                         old system call
		tt == 0x106 || //                                                nfssys
		tt == 0x108 || // Solaris Syscall             ILP32 system call on LP64
		tt == 0x110 || // Linux 32bit Syscall             V9 user trap handlers
		tt == 0x111 || // Old Linux 64bit Syscall                     "
// short execution, don't count it:
//		tt == 0x124 || //                                         get timestamp
//		tt == 0x126 || //                                            self xcall
// short execution, don't count it:
//		tt == 0x127 || // indirect Solaris Syscall                 get hrestime
		
		tt == 0x140 || //                                      LP64 system call
		tt == 0x16d // Linux 64bit Syscall
		);
}

hw_context_t *
thread_scheduler_t::search_hw_context(sequencer_t *_seq, uint32 _ct)
{
    for (uint32 i = 0; i < num_hw_ctxt; i++)
        if (hw_context[i]->seq == _seq && hw_context[i]->ctxt == _ct)
            return hw_context[i];
    FAIL_MSG("Cannot find hardware context");
    return 0;
}

