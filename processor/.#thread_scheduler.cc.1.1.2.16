/* Copyright (c) 2005 by Gurindar S. Sohi for the Wisconsin
 * Multiscalar Project.  ALL RIGHTS RESERVED.
 *
 * This software is furnished under the Multiscalar license.
 * For details see the LICENSE.mscalar file in the top-level source
 * directory, or online at http://www.cs.wisc.edu/mscalar/LICENSE
 *
 */

/* description:    thread scheduler for mapping threads to sequencers
 * initial author: Philip Wells 
 *
 */
 
#include "simics/first.h"
RCSID("$Id: thread_scheduler.cc,v 1.1.2.16 2005/11/25 22:00:48 kchak Exp $");

#include "definitions.h"
#include "isa.h"
#include "dynamic.h"
#include "proc_stats.h"
#include "chip.h"
#include "stats.h"
#include "counter.h"
#include "mai_instr.h"
#include "sequencer.h"
#include "verbose_level.h"
#include "config_extern.h"
#include "histogram.h"
#include "mai.h"
#include "sequencer.h"

#include "thread_scheduler.h"

hw_context_t::hw_context_t (sequencer_t *_s, uint32 _ct, bool _b)
    : seq(_s), ctxt(_ct), busy(_b), curr_sys_call(0)
{
    
}
        

thread_scheduler_t::thread_scheduler_t(chip_t *_chip)
{
	chip = _chip;
	
	num_user = g_conf_num_user_seq;
	num_hw_ctxt = 0;
    
	num_threads = chip->get_num_cpus();
    preferred_user_ctxt = new uint32[num_threads];
    preferred_os_ctxt   = new uint32[num_threads];
    next_hw_context_assignment = g_conf_num_user_seq;
    
    
    if (g_conf_chip_design[0]) {
        uint32 count = 0;
        num_seq = g_conf_chip_design[0];
        for (uint32 i = 1; i <= num_seq; i++)
            num_hw_ctxt += g_conf_chip_design[i];
        printf("Number of hardware contexts = %d\n", num_hw_ctxt);
        hw_context = new (hw_context_t *)[num_hw_ctxt];
        
        for (uint32 i = 1 ; i <= num_seq; i++)
        {
            for (uint32 j = 0; j < g_conf_chip_design[i]; j++, count++)
                hw_context[count] = new hw_context_t(chip->get_sequencer(i - 1), 
                    j, (count < num_threads));
        }
        
        // Provision extra h/w contexts for OS
        for (uint32 i = num_threads; i < num_hw_ctxt; i++)
            idle_os_ctxt.push_back(hw_context[i]);
        
        
    } else {
        num_hw_ctxt = num_threads;
        hw_context = new hw_context_t *[num_hw_ctxt];
        for (uint32 i = 0; i < num_hw_ctxt; i++)
            hw_context[i] = new hw_context_t (chip->get_sequencer(i), 0, true);
    }
    
    if (g_conf_syscall_provision[0]) provision_syscall();
    
    // Set  up the user ctxt
    if (g_conf_num_user_seq) {
        uint32 curr_ctxt = 0;
        for (uint32 i = 0; i < num_threads; i++)
        {
            preferred_user_ctxt[i] = curr_ctxt++;
            if (curr_ctxt == g_conf_num_user_seq) curr_ctxt = 0;
        }
        curr_ctxt = g_conf_num_user_seq;
        for (uint32 i = 0; i < num_threads; i++)
        {
            preferred_os_ctxt[i] = curr_ctxt++;
            if (curr_ctxt == num_hw_ctxt) curr_ctxt = g_conf_num_user_seq;
        }
            
    }
    
    last_os_mapping_switch = 100;
    last_user_mapping_switch = 100;
    next_os_thread_switch = 0;
    next_user_thread_switch = 0;
    if (g_conf_num_user_seq) {
        require_os_hop = (num_threads % (num_hw_ctxt - g_conf_num_user_seq) != 0);
        require_user_hop = (num_threads % g_conf_num_user_seq != 0);
    } else {
        require_os_hop = false;
        require_user_hop = false;
    }
   
    
}


void
thread_scheduler_t::provision_syscall()
{
    ASSERT(g_conf_num_user_seq);
    uint32 count = g_conf_syscall_provision[0];
    uint32 syscall, context;
    for (uint32 i = 0; i < count; i++)
    {
        syscall = g_conf_syscall_provision[ i * 2 + 1];
        context = g_conf_syscall_provision [ i * 2 + 2];
        DEBUG_OUT("Assigning syscall %d to %d contexts\n", syscall, context);
        assign_new_syscall_ctxt(syscall, context);
    }
    
    if (g_conf_cache_only_provision) {
        dynamic_provision_offset = next_hw_context_assignment;
    }
    
}

void
thread_scheduler_t::assign_new_syscall_ctxt(uint32 syscall, uint32 context)
{
    syscall_context[syscall].insert(hw_context[context]);
}


bool
thread_scheduler_t::thread_yield(sequencer_t *seq, uint32 ctxt, mai_t *mai,
	ts_yield_reason_t why)
{
	// A currently running sequencer thinks its thread may need to switch
	// Check for sure if this is what we want to do and do it.

	if (!g_conf_separate_user_os) 
		return false;

	bool user_seq = is_user_ctxt(seq, ctxt);

	switch(why) {
		case YIELD_LONG_RUNNING:
			if (user_seq && !wait_for_user.empty())
				return true;
			if (!user_seq && !wait_for_os.empty())
				return true;
			break;
	
		case YIELD_MUTEX_LOCKED:
			ASSERT(mai->is_supervisor());
			if (!wait_for_os.empty())
				return true;
			break;
	
		case YIELD_DONE_RETRY:
			// returned to user
			if (!mai->is_supervisor() && !user_seq)
				return true;
			break;

		case YIELD_EXCEPTION:
			ASSERT(mai->get_tl() > 0);
			ASSERT(mai->is_supervisor());
			if (mai->is_syscall_trap(mai->get_tt(mai->get_tl())) ||
                mai->is_interrupt_trap(mai->get_tt(mai->get_tl())))
            {
				return true;
            }
			break;
			
		case YIELD_IDLE_ENTER:
			ASSERT(mai->is_supervisor());
			if (!wait_for_os.empty())
				return true;
			break;
	
		default:
			FAIL;
			return false;
	}

	return false;
}


hw_context_t *thread_scheduler_t::match_user_context(mai_t *mai)
{
    hw_context_t *ret = 0;
    // Policy :-
    // Choose (if possible) the context the thread used last time
        
    uint32 preferred_context = preferred_user_ctxt[mai->get_id()];
    if (hw_context[preferred_context]->busy == false)
    {
        ret = hw_context[preferred_context];   
    }
    else if (idle_user_ctxt.size())
    {
        ret = idle_user_ctxt.front();
        idle_user_ctxt.pop_front();
    }
    
    return ret;
}

void thread_scheduler_t::debug_idle_context(list<hw_context_t *> l)
{
    list<hw_context_t *>::iterator it = l.begin();
    while (it != l.end())
    {
        hw_context_t *hwc = *it;
        DEBUG_OUT("seq = %d ctxt = %d\n", hwc->seq->get_id(), hwc->ctxt);
        it++;
    }
}

hw_context_t *thread_scheduler_t::get_syscall_context(uint32 syscall)
{
    
    hw_context_t *ret = 0;
    map<uint32, set<hw_context_t *> >::iterator it = syscall_context.find(syscall);
    if (it != syscall_context.end())
    {
        set<hw_context_t *> ctxt_set = it->second;
        set<hw_context_t *>::iterator c_it = ctxt_set.begin();
        while (c_it != ctxt_set.end())
        {
            hw_context_t *desired = *c_it;
            if (!desired->busy)
            {
                //debug_idle_context(idle_os_ctxt);
                ASSERT(find(idle_os_ctxt.begin(), idle_os_ctxt.end(), desired)
                    != idle_os_ctxt.end());
                ret = desired;
                idle_os_ctxt.remove(desired);
                break;
            }
            c_it++;
        }
            
        ASSERT(idle_os_ctxt.size() < (num_hw_ctxt - g_conf_num_user_seq)); 
        
    } else {
        // First time occurence map to the most easily available
        if (!idle_os_ctxt.empty()) {
            ret = idle_os_ctxt.front();
            syscall_context[syscall].insert(ret);
            idle_os_ctxt.pop_front();
        } else {
            assign_new_syscall_ctxt(syscall, 1);
        }
    }
    
    return ret;
    
}
    
hw_context_t *thread_scheduler_t::maximize_cache_ctxt(mai_t *mai, bool desire_user_ctxt,  
    uint32 syscall)
{
    hw_context_t *ret = 0;
    if (!desire_user_ctxt)
    {
        ret = get_syscall_context(syscall);   
    } else {
        hw_context_t *preferred = hw_context[preferred_user_ctxt[mai->get_id()]];
        if (!preferred->busy) 
        {
            ASSERT(find(idle_user_ctxt.begin(), idle_user_ctxt.end(), preferred)
                    != idle_user_ctxt.end());
            ret = preferred;
            idle_user_ctxt.remove(preferred);
        }
    }
    
    return ret;
}

mai_t *thread_scheduler_t::maximize_cache_thread(hw_context_t *hwc, bool user_ctxt)
{
    mai_t *ret = 0;
    if (user_ctxt && !wait_for_user.empty())
    {
        list<mai_t *>::iterator it = wait_for_user.begin();
        while (it != wait_for_user.end())
        {
            mai_t *candidate = *it;
            if (hwc == hw_context [preferred_user_ctxt [candidate->get_id()] ])
            {
                ret = candidate;
                wait_for_user.erase(it);
                break;
            }
            it++;
        }
    } else if (!user_ctxt && !wait_for_os.empty()) {
        list<mai_t *>::iterator it = wait_for_os.begin();
        while (it != wait_for_os.end())
        {
            mai_t *candidate = *it;
            uint32 syscall = candidate->get_syscall_num();
            map<uint32, set<hw_context_t *> >::iterator syscall_it = syscall_context.find(syscall);
            ASSERT(syscall_it != syscall_context.end());
            if (syscall_it->second.find(hwc) != syscall_it->second.end())
            {
                ret = candidate;
                wait_for_os.erase(it);
                break;
            }
            it++;
        }   
    }
    
    return ret;
}

hw_context_t *thread_scheduler_t::eager_fifo_ctxt(mai_t *mai, bool desire_user_ctxt, uint32 syscall)
{
    // Eager scheduling: return h.w context in FIFO manner
    
    hw_context_t *ret = 0;
    if (!desire_user_ctxt && !idle_os_ctxt.empty())
    {
        ret = idle_os_ctxt.front();
        idle_os_ctxt.pop_front();
    } else if (desire_user_ctxt && !idle_user_ctxt.empty()) {
        ret = idle_user_ctxt.front();
        idle_user_ctxt.pop_front();
    }
    
    return ret;
    
}
        
mai_t *thread_scheduler_t::eager_fifo_thread(hw_context_t *hwc, bool user_ctxt)
{
    mai_t *ret = 0;
    if (user_ctxt && !wait_for_user.empty()) {
		ret = wait_for_user.front();
		wait_for_user.pop_front();

	} else if (!user_ctxt && !wait_for_os.empty()) {
		ret = wait_for_os.front();
		wait_for_os.pop_front();
	}
    
    return ret;
    
}

hw_context_t *thread_scheduler_t::find_ctxt_for_thread(mai_t *mai, bool desire_user_ctxt,
    uint32 syscall)
{
    
    hw_context_t *ret = NULL;
	
    switch (g_conf_scheduling_algorithm) {
        case 0:
            ret = eager_fifo_ctxt(mai, desire_user_ctxt, syscall);
            break;
        case 1:
            ret = maximize_cache_ctxt(mai, desire_user_ctxt, syscall);
            break;
        default:
            FAIL_MSG("Invalid Scheduling Specified");
    }
            
    // Update current syscall info
    if (ret && !desire_user_ctxt) ret->curr_sys_call = syscall;
    
    return ret;
}

mai_t *thread_scheduler_t::find_thread_for_ctxt(hw_context_t *hwc, bool user_ctxt)
{
    mai_t *ret = 0;
    
    switch (g_conf_scheduling_algorithm) {
        case 0:
            ret = eager_fifo_thread(hwc, user_ctxt);
            break;
        case 1:
            ret = maximize_cache_thread(hwc, user_ctxt);
            break;
        default:
            FAIL_MSG("Invalid Scheduling Specified");
    }
    
    return ret;
    
}

void thread_scheduler_t::silent_switch(sequencer_t *seq, uint32 ctxt, mai_t *mai)
{
	bool desire_user_seq = !mai->is_supervisor();  // may == user_seq if preemted

	FE_EXCEPTION_CHECK;
	// find idle seq, or put thread on idle list
    uint32 syscall = 0;
    hw_context_t *preferred_ctxt;
    if (!desire_user_seq) syscall = mai->get_syscall_num();
    
    if (desire_user_seq)
    {
        preferred_ctxt = get_preferred_user_context(mai->get_id());
        
    } else {
        // Assume single mapping;
        
        map<uint32, set<hw_context_t *> >::iterator it = syscall_context.find(syscall);
        if (it != syscall_context.end())
        {
            //ASSERT(it->second.size() == 1); // Can be changed later
            uint32 way = mai->get_id() % it->second.size();
            if (it->second.size() == 1 || way == 0) 
                preferred_ctxt = *(it->second.begin());
            else {
                set<hw_context_t *>::iterator c_it = it->second.begin();
                for (; way > 0; way--,c_it++)
                { 
                    // do nothing
                }
                preferred_ctxt = *c_it;
            }
        } else {
            preferred_ctxt = hw_context[next_hw_context_assignment++];
            // Catch All - code offset to the as much as provisioned
            //if (next_hw_context_assignment == num_hw_ctxt)
            //    next_hw_context_assignment = dynamic_provision_offset;
            if (next_hw_context_assignment == num_hw_ctxt)
                next_hw_context_assignment = g_conf_num_user_seq;
            syscall_context[syscall].insert(preferred_ctxt);
        }
        
        if (g_conf_thread_provision)
            preferred_ctxt = get_preferred_os_context(mai->get_id());
        
    }
    
    
    seq->set_mem_hier_seq(preferred_ctxt->seq, ctxt);
    
}


hw_context_t *thread_scheduler_t::get_preferred_os_context(uint32 t_id)
{
 
    if (!require_os_hop) return hw_context[preferred_os_ctxt[t_id]];
    uint32 ctxt  = 0;
    tick_t curr_cycle = chip->get_g_cycles();
     if (curr_cycle > last_os_mapping_switch &&
        (curr_cycle - last_os_mapping_switch > (uint32)g_conf_thread_hop_interval)) {
        uint32 num_hops = get_num_hops(num_hw_ctxt - g_conf_num_user_seq);
        for (uint32 i = 0; i < num_hops; i++)
        {
            uint32 home = preferred_os_ctxt[next_os_thread_switch];
            uint32 residual = (home + num_hops) % num_hw_ctxt;
            preferred_os_ctxt[next_os_thread_switch] = (home + num_hops >= num_hw_ctxt) 
                ? g_conf_num_user_seq + residual: (home + num_hops) ;
            next_os_thread_switch = (next_os_thread_switch + 1) % num_threads;
        }
        last_os_mapping_switch = curr_cycle;
    }
    
    for (uint32 i = 0; i < num_threads; i++)
        ASSERT(preferred_os_ctxt[i] >= g_conf_num_user_seq &&
               preferred_os_ctxt[i] < num_hw_ctxt);
    //DEBUG_OUT("OS thread %u mapped to %u\n", i, preferred_os_ctxt[i]);
    //DEBUG_OUT("====================\n");
    
    ctxt = preferred_os_ctxt[t_id];
    return hw_context[ctxt];
}

uint32 thread_scheduler_t::get_num_hops(uint32 resources)
{
    // Assume that number of threads == 8
    switch(resources) {
        case 3:
            return 2;
            break;
        case 5:
            return 3;
            break;
        case 6:
            return 2;
            break;
        case 7:
            return 1;
            break;
        default:
            FAIL;
    }
    
    //uint32 ret = num_threads % resources;
    //if (ret > resources/2) ret = resources - ret;
    //return ret;
}

hw_context_t *thread_scheduler_t::get_preferred_user_context(uint32 t_id)
{
    if (!require_user_hop) return hw_context[preferred_user_ctxt[t_id]];
    tick_t curr_cycle = chip->get_g_cycles();
    if (curr_cycle > last_user_mapping_switch &&
        (curr_cycle - last_user_mapping_switch) > (uint32)g_conf_thread_hop_interval)
    {
        uint32 num_hops = get_num_hops(g_conf_num_user_seq);
        for (uint32 i = 0; i < num_hops; i++)
        {
            uint32 home = preferred_user_ctxt[next_user_thread_switch];
            preferred_user_ctxt[next_user_thread_switch] = (home + num_hops) % g_conf_num_user_seq;
            next_user_thread_switch = (next_user_thread_switch + 1) % num_threads;
        }
        last_user_mapping_switch = curr_cycle;
    }
    for (uint32 i = 0; i < num_threads; i++)
        ASSERT(preferred_user_ctxt[i] < g_conf_num_user_seq);
    
    return hw_context[preferred_user_ctxt[t_id]];
}


void
thread_scheduler_t::ready_for_switch(sequencer_t *seq, uint32 ctxt, mai_t *mai,
	ts_yield_reason_t why)
{
	
	// Something may have changed, and we now don't want to switch
	if (!thread_yield(seq, ctxt, mai, why))
		return;
    
    if (g_conf_cache_only_provision) {
        silent_switch(seq, ctxt, mai);
        return;
    }
	
	bool user_ctxt = is_user_ctxt(seq, ctxt); 
	bool desire_user_seq = !mai->is_supervisor();  // may == user_seq if preemted

	
	// find idle seq, or put thread on idle list
    
    uint32 syscall = 0;
    if (!desire_user_seq) syscall = mai->get_syscall_num();
    
    ASSERT(desire_user_seq || syscall);
    
    //DEBUG_OUT("%8llu: Switching thread %d From seq %d ctxt %d need %u\n",  chip->get_g_cycles(), 
    //    mai->get_id(), seq->get_id(), ctxt, desire_user_seq);   
    
    hw_context_t *hw_ctxt = search_hw_context(seq, ctxt);
    
    hw_context_t *new_ctxt = find_ctxt_for_thread(mai, desire_user_seq, syscall);
    
	if (new_ctxt) {
        //DEBUG_OUT("%8llu: Thread id %d gets seq %d ctxt %d\n", chip->get_g_cycles(),
        //mai->get_id(), new_ctxt->seq->get_id(), new_ctxt->ctxt);
            
		chip->switch_to_thread(new_ctxt->seq, new_ctxt->ctxt, mai);
        new_ctxt->busy = true;
        //debug_idle_context(idle_os_ctxt);
	} else {
		if (desire_user_seq)
			wait_for_user.push_back(mai);
		else
			wait_for_os.push_back(mai);
        //DEBUG_OUT("%8llu: Thread %d going idle requires %u syscall %u\n", 
        //     chip->get_g_cycles(), mai->get_id(), desire_user_seq, syscall);
        
        chip->idle_thread_context(mai->get_id());
	}

	// check for matching idle thread to run on newly opened seq	
	// or put old seq on idle list
	mai_t *new_thread = find_thread_for_ctxt(hw_ctxt, user_ctxt);
    
	if (!new_thread) {
        //DEBUG_OUT("%8llu : Idling seq %d ctxt %d \n", chip->get_g_cycles(), 
        //    hw_ctxt->seq->get_id(), hw_ctxt->ctxt);
        hw_ctxt->busy = false;
		if (user_ctxt)
			idle_user_ctxt.push_back(hw_ctxt);
		else {
			idle_os_ctxt.push_back(hw_ctxt);
        //    debug_idle_context(idle_os_ctxt);
        }
	} else {
        //DEBUG_OUT("%8llu: Thread %d gets seq %d ctxt %d\n", 
        //    chip->get_g_cycles(), new_thread->get_id(), hw_ctxt->seq->get_id(), 
        //    hw_ctxt->ctxt);
        hw_ctxt->busy = true;    
    }

    ASSERT(wait_for_os.size() + wait_for_user.size() < num_threads);
   // if (chip->get_g_cycles() % 10000 == 0)
   // DEBUG_OUT("%8llu: IDLE OS ctxt %d IDLE USER CTXT %d IDLE OS THREAD %d IDLE USER THREAD %d\n",
   //     chip->get_g_cycles(), idle_os_ctxt.size(), idle_user_ctxt.size(), wait_for_os.size(),
   //         wait_for_user.size());
	chip->switch_to_thread(seq, ctxt, new_thread);
    debug_user_ctxt();
			
}

void thread_scheduler_t::debug_user_ctxt()
{
    uint32 size = idle_user_ctxt.size();
    uint32 count = 0;
    for (uint32 i = 0; i < g_conf_num_user_seq; i++)
    {
        if (hw_context[i]->busy == false) count++;
    }
    ASSERT(count == size);
}


void 
thread_scheduler_t::write_checkpoint(FILE *file)
{
    fprintf(file, "%llu %llu %u %u\n", last_user_mapping_switch,
      last_os_mapping_switch, next_user_thread_switch, next_os_thread_switch);
    // Put the OS contexts first
    list<hw_context_t *>::iterator it;
    hw_context_t *hwc;
    
    
    fprintf(file, "%u\n", idle_os_ctxt.size());
    for (it = idle_os_ctxt.begin(); it != idle_os_ctxt.end(); it++)
    {
        hwc = *it;
        ASSERT(hwc->seq->get_mai_object(hwc->ctxt) == 0);
        fprintf(file, "%u %u\n", hwc->seq->get_id(), hwc->ctxt);
    }
    
    fprintf(file, "%u\n", idle_user_ctxt.size());
    for (it = idle_user_ctxt.begin(); it != idle_user_ctxt.end(); it++)
    {
        hwc = *it;
        ASSERT(hwc->seq->get_mai_object(hwc->ctxt) == 0);
        fprintf(file, "%u %u\n", hwc->seq->get_id(), hwc->ctxt);
    }
    
    // Now the idle cpus
    list<mai_t *>::iterator t_it;
    fprintf(file, "%u\n", wait_for_os.size());
    for (t_it = wait_for_os.begin(); t_it != wait_for_os.end(); t_it++)
    {
        mai_t *thread = *t_it;
        fprintf(file, "%u\n", thread->get_id());
    }
    
    fprintf(file, "%u\n", wait_for_user.size());
    for (t_it = wait_for_user.begin(); t_it != wait_for_user.end(); t_it++)
    {
        mai_t *thread = *t_it;
        fprintf(file, "%u\n", thread->get_id());
    }
    
    // Now syscall provisions
    fprintf(file, "%u\n", syscall_context.size());
    
    map<uint32, set<hw_context_t *> > ::iterator sit;
    for (sit = syscall_context.begin(); sit != syscall_context.end(); sit++)
    {
        uint32 syscall = sit->first;
        set<hw_context_t *> hwc_set = sit->second;
        fprintf(file, "%u %u ", syscall, hwc_set.size());
        set<hw_context_t *>::iterator i;
        for (i = hwc_set.begin(); i != hwc_set.end(); i++)
        {
            fprintf(file, "%u ", get_context_id(*i));
        }
        fprintf(file , "\n");
    }
    
    // Hw_context busy flag
    for (uint32 i = 0; i < num_hw_ctxt; i++)
        fprintf(file, "%u\n", hw_context[i]->busy);
    
}

void thread_scheduler_t::read_checkpoint(FILE *file)
{
    
    fscanf(file, "%llu %llu %u %u\n", &last_user_mapping_switch,
      &last_os_mapping_switch, &next_user_thread_switch, &next_os_thread_switch);
    
    // clear out the idle context list
    idle_os_ctxt.clear();
    idle_user_ctxt.clear();
    syscall_context.clear();
    
    list<hw_context_t *>::iterator it;
    hw_context_t *hwc;
 
    uint32 idle_ctxt, seq_id, _ct;
    fscanf(file, "%u\n", &idle_ctxt);
    for (uint32 i = 0; i < idle_ctxt; i++)
    {
        fscanf(file, "%u %u\n", &seq_id, &_ct);
        hwc = search_hw_context(chip->get_sequencer(seq_id), _ct);
        ASSERT(chip->get_sequencer(seq_id)->get_mai_object(_ct) == NULL);
        idle_os_ctxt.push_back(hwc);
    }
    
    fscanf(file, "%u\n", &idle_ctxt);
    for (uint32 i = 0; i < idle_ctxt; i++)
    {
        fscanf(file, "%u %u\n", &seq_id, &_ct);
        ASSERT(chip->get_sequencer(seq_id)->get_mai_object(_ct) == NULL);
        hwc = search_hw_context(chip->get_sequencer(seq_id), _ct);
        idle_user_ctxt.push_back(hwc);
    }
    
    uint32 thread_id, idle_threads;
    fscanf(file, "%u\n", &idle_threads);
    for (uint32 i = 0; i < idle_threads; i++)
    {
        fscanf(file , "%u\n", &thread_id);
        mai_t *thread = chip->get_mai_object(thread_id);
        wait_for_os.push_back(thread);
    }
    
    fscanf(file, "%u\n", &idle_threads);
    for (uint32 i = 0; i < idle_threads; i++)
    {
        fscanf(file , "%u\n", &thread_id);
        mai_t *thread = chip->get_mai_object(thread_id);
        wait_for_user.push_back(thread);
    }
    
    uint32 count;
    fscanf(file, "%u\n", &count);
    for (uint32 i = 0; i < count; i++)
    {
        uint32 syscall, size, ctxt;
        fscanf(file, "%u %u ", &syscall, &size);
        for (uint32 j = 0; j < size; j++)
        {
            fscanf(file, "%u ", &ctxt);
            syscall_context[syscall].insert(hw_context[ctxt]);
        }
        fscanf(file ,"\n");
    }
    
    for (uint32 i = 0; i < num_hw_ctxt; i++)
    {
        uint32 busy;
        fscanf(file, "%u\n", &busy);
        hw_context[i]->busy = busy ? true : false;
    }
    
}

bool
thread_scheduler_t::is_user_ctxt(sequencer_t *_s, uint32 _c) {
    // Hack for cache_only_provision
    if (g_conf_cache_only_provision) _s = _s->get_mem_hier_seq(_c);
    for (uint32 i = 0; i < num_hw_ctxt; i++)
        if (hw_context[i]->seq == _s && hw_context[i]->ctxt == _c)
            return (i < num_user);
    FAIL;
}

bool
thread_scheduler_t::is_os_ctxt(sequencer_t *_s, uint32 _c) {
	for (uint32 i = 0; i < num_hw_ctxt; i++)
        if (hw_context[i]->seq == _s && hw_context[i]->ctxt == _c)
            return (i >= num_user);
    FAIL;
}

hw_context_t *
thread_scheduler_t::search_hw_context(sequencer_t *_seq, uint32 _ct)
{
    for (uint32 i = 0; i < num_hw_ctxt; i++)
        if (hw_context[i]->seq == _seq && hw_context[i]->ctxt == _ct)
            return hw_context[i];
    FAIL_MSG("Cannot find hardware context");
    return 0;
}

uint32 
thread_scheduler_t::get_context_id(hw_context_t *hwc)
{
    for (uint32 i = 0; i < num_hw_ctxt; i++)
        if (hw_context[i] == hwc) return i;
    FAIL_MSG ("Invalid context id passed");
    return num_hw_ctxt;
}

