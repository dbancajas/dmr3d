/* Copyright (c) 2003 by Gurindar S. Sohi for the Wisconsin
 * Multiscalar Project.  ALL RIGHTS RESERVED.
 *
 * This software is furnished under the Multiscalar license.
 * For details see the LICENSE.mscalar file in the top-level source
 * directory, or online at http://www.cs.wisc.edu/mscalar/LICENSE
 *
 */

/* description:    main class for the memory hierarchy
 * initial author: Philip Wells 
 *
 */
 
#include "simics/first.h"
RCSID("$Id: mem_hier.cc,v 1.13.2.14 2005/07/29 20:38:09 pwells Exp $");

#include "definitions.h"
#include "config_extern.h"
#include "mem_hier.h"
#include "device.h"
#include "link.h"
#include "statemachine.h"
#include "proc.h"
#include "mainmem.h"
#include "cache.h"
#include "tcache.h"
#include "iodev.h"
#include "transaction.h"
#include "mem_hier_handle.h"
#include "window.h"
#include "meventq.h"

#include "config.h"
#include "counter.h"
#include "histogram.h"
#include "stats.h"

#include "isa.h"
#include "dynamic.h"
#include "trans_rec.h"
#include "random_tester.h"


// Definitions of static objects
mem_hier_handle_iface_t *mem_hier_t::mhh;
mem_hier_t *mem_hier_t::g_ptr;

mem_hier_t::mem_hier_t(proc_object_t *_module)
	: module_obj(_module)
{
	num_deferred = 0;
	num_io_writes = 0;
	stall_delta = 1;
	runtime_config_file = "";
	config = NULL;
	is_initialized = false;

	num_devices = 0;
	num_links = 0;
	links = NULL;
	devices = NULL;
	
	eventq = NULL;
	g_cycles = 0;

}

// Must be called after the simics configuration file has set all required
// attributes
void
mem_hier_t::init() {

    //for (uint32 idiot=0; idiot < 900000000; idiot++) {}
	// Create eventq
	eventq = new meventq_t(g_conf_main_mem_latency+g_conf_random_mm_dist+2);
	external::post_event(this);
	
	// Init random generator
	srandom(g_conf_random_seed);

	currently_idle = new bool[num_processors];
	idle_start = new tick_t[num_processors];
    initial_step_count = new uint64[num_processors];
	for (uint32 i = 0; i < num_processors; i++) {
		currently_idle[i] = false;
		idle_start[i] = 0;
        initial_step_count[i] = SIM_step_count(SIM_proc_no_2_ptr(i));
        printf("Initial Step Count set to %llu\n", initial_step_count[i]);
	}

    if (g_conf_mem_hier_response_tracking)
        tr_rec = new trans_rec_t(num_processors);
    
    if (g_conf_random_addresses)
        random_tester = new random_tester_t(this);
	// Create stats
	stats = new stats_t();
	stats_print_enqueued = false;
	
	stat_cycles = stats->COUNTER_BASIC("mh_cycles", "# of cycles");
	stat_num_requests = stats->COUNTER_BASIC("num_requests", "# of requests received by mem hier");
	stat_num_reads = stats->COUNTER_BASIC("num_reads", "# of read requests");
	stat_num_writes = stats->COUNTER_BASIC("num_writes", "# of write requests");
	stat_num_atomics = stats->COUNTER_BASIC("num_atomics", "# of atomic requests");
	stat_num_io_space = stats->COUNTER_BASIC("num_io_space", "# of requests to/from I/O space");
	stat_num_io_device = stats->COUNTER_BASIC("num_io_device", "# of requests from a I/O device");
	stat_num_io_device_phys_space = stats->COUNTER_BASIC("num_io_device_phys_space", "# of requests from a I/O device to phys space");
	stat_num_uncache = stats->COUNTER_BASIC("num_uncache", "# of uncacheable requests");
	stat_num_uncache_io_space = stats->COUNTER_BASIC("num_uncache_io_space", "# of uncacheable requests to/from I/O space");
	stat_num_uncache_io_device = stats->COUNTER_BASIC("num_uncache_io_device", "# of uncacheable requests from I/O device");
	stat_num_may_not_stall = stats->COUNTER_BASIC("num_may_not_stall", "# of requests with may_stall unset");
	stat_num_squash = stats->COUNTER_BASIC("num_squash", "# of requests squashed before issued to mem-hier");

	stat_request_size = stats->HISTO_EXP2("request_size", "distribution of request sizes",
	                               8, 1, 1);
								   
	stat_requesttime_histo = stats->HISTO_EXP2("request_time",
											   "distribution of miss latencies",
											   8,1,1);
	stat_cpu_histo = stats->HISTO_UNIFORM("cpu_histo",
										  "distribution of requests from each cpu",
										  num_processors,1,0);
//	stat_asi_histo = stats->HISTO_UNIFORM("asi_histo",
//										  "distribution of requests from each ASI",
//										  0xff,1,0);
	stat_idle_cycles_histo = stats->COUNTER_GROUP("idle_cycles",
		"cycles each cpu spends in idle loop", num_processors);

	stat_l1_hit = stats->COUNTER_BASIC("l1_hit", "# of l1 hits");
	stat_l1_miss = stats->COUNTER_BASIC("l1_miss", "# of l1 demand+coherence misses");
	stat_l1_coher_miss = stats->COUNTER_BASIC("l1_coher_miss", "# of l1 coherence misses");
	stat_l1_mshr_part_hit = stats->COUNTER_BASIC("l1_mshr_part_hit", "# of l1 mshr hits");
	stat_l1_stall = stats->COUNTER_BASIC("l1_stall", "# of l1 stalls");

	stat_l2_hit = stats->COUNTER_BASIC("l2_hit", "# of l2 hits");
	stat_l2_miss = stats->COUNTER_BASIC("l2_miss", "# of l2 demand+coherence misses");
	stat_l2_coher_miss = stats->COUNTER_BASIC("l2_coher_miss", "# of l2 coherence misses");
	stat_l2_mshr_part_hit = stats->COUNTER_BASIC("l2_mshr_part_hit", "# of l2 mshr hits");
	stat_l2_req_fwd = stats->COUNTER_BASIC("l2_req_fwd", "# of l2 request forwards");
	stat_l2_stall = stats->COUNTER_BASIC("l2_stall", "# of l2 stalls");

	create_network();

	is_initialized = true;
    // Checkpoint initiliazation
    checkpoint_taken = false;
    if (g_conf_mem_hier_checkpoint_in != "") {
        cout << "Reading checkpoint from " << g_conf_mem_hier_checkpoint_in << endl;
        read_checkpoint(g_conf_mem_hier_checkpoint_in);
        print_stats();
    }

	config->print();
}

mem_hier_handle_iface_t *
mem_hier_t::get_handler() {
	return mhh;
}

void
mem_hier_t::set_handler(mem_hier_handle_iface_t *_mhh) {
	mhh = _mhh;
}

void
mem_hier_t::set_cpus(conf_object_t **_cpus, uint32 _num) {
	cpus = _cpus;
	num_processors = _num;
}

conf_object_t **
mem_hier_t::get_cpus() {
	return cpus;
}

config_t *
mem_hier_t::get_config_db() {
	return config;
}

meventq_t *
mem_hier_t::get_eventq() {
	return eventq;
}

tick_t
mem_hier_t::get_g_cycles() {
	return g_cycles;
}

conf_object_t *
mem_hier_t::get_cpu_object(uint32 id) {
	ASSERT(id < num_processors);
	return (cpus[id]);
}

generic_proc_t *
mem_hier_t::get_proc_object(conf_object_t *cpu) {
	for (uint32 i = 0; i < num_processors; i++) {
		if (cpus[i] == cpu) return static_cast <generic_proc_t *> (devices[i]);
	}
	FAIL_MSG("cpu object not found: %s", cpu->name);
}

uint32
mem_hier_t::get_num_processors() {
	return num_processors;
}

void
mem_hier_t::read_runtime_config_file(string file) {

	if (config) delete config;

	config = new config_t();
	config->register_entries();
	if (file != "") config->parse_runtime(file);
}

void
mem_hier_t::set_ptr(mem_hier_t *_ptr) {
	g_ptr = _ptr;
}

mem_hier_t *
mem_hier_t::ptr() {
	return g_ptr;
}


// Create the links and devices for the memory hierarchy network
//   TODO: This should all be parameterized or read in from a file
void
mem_hier_t::create_network() 
{
	init_network();
	if (g_conf_memory_topology == "unip-one-default")
		create_unip_one_default();
	else if (g_conf_memory_topology == "minimal-default")
		create_minimal_default();
	else if (g_conf_memory_topology == "mp-minimal-default")
		create_mp_minimal_default();
	else if (g_conf_memory_topology == "unip-one-bus")
		create_unip_one_bus();
	else if (g_conf_memory_topology == "msi-one-bus")
		create_mp_bus();
	else if (g_conf_memory_topology == "unip-one-mp-msi")
		create_unip_one_mp_msi();
	else if (g_conf_memory_topology == "unip-two-default")
		create_unip_two_default();
	else if (g_conf_memory_topology == "msi-two-bus")
		create_two_level_mp_bus();
	else if (g_conf_memory_topology == "msi-two-split")
		create_two_level_split_mp();
	else if (g_conf_memory_topology == "unip-one-cachedata")
		create_unip_one_cachedata();
	else if (g_conf_memory_topology == "cmp_incl")
        create_cmp_incl();
	else if (g_conf_memory_topology == "cmp_excl")
        create_cmp_excl();
 	else if (g_conf_memory_topology == "cmp_excl_3l")
        create_cmp_excl_3l();
    else
		ASSERT_MSG(0, "UNDEFINED PROTOCOL");
}

void
mem_hier_t::advance_cycle()
{
	STAT_INC(stat_cycles);
	g_cycles++;
	eventq->advance_cycle();
	
	if (g_conf_stats_print_cycle &&
	    g_cycles % g_conf_stats_print_cycle == 0)
	{
		print_stats();
	}
    
    if (g_conf_mem_hier_response_tracking && 
        g_cycles % g_conf_mem_hier_check_frequency == 0)
    {
        tr_rec->detect_error(g_cycles);
    }
    
    if (g_conf_mem_hier_warmup && g_cycles > (uint64)g_conf_mem_hier_warmup)
	{
		clear_stats();
		g_conf_mem_hier_warmup = 0;
	}
	
	// XXX TEMP!
	if (g_conf_copy_transaction) {
		while (!delete_list.empty() &&
			(tick_t)delete_list.front()->request_time + 
			g_conf_mem_hier_response_threshold < g_cycles)
		{
			mem_trans_t *trans = delete_list.front();
			delete_list.pop_front();
			delete trans;
		}
	}
    
    if (!g_conf_use_processor) handle_simulation();
}


void mem_hier_t::handle_simulation()
{
    uint64 total_commits = 0;
    uint64 total_cycles = g_cycles;


    for (uint32 i = 0; i < num_processors; i++) {
        total_commits += SIM_step_count(SIM_proc_no_2_ptr(i)) - initial_step_count[i];
    }
    
    if ((g_conf_run_commits && total_commits >= g_conf_run_commits) ||
        (g_conf_run_cycles && total_cycles >= g_conf_run_cycles))
    {
        //print_stats();
        SIM_break_simulation("Done");
    }
    
    
}


void
mem_hier_t::fix_cycle(uint64 cycle)
{
    g_cycles = cycle;
    
}

void
mem_hier_t::complete_request(conf_object_t *cpu, mem_trans_t *trans)
{
	VERBOSE_OUT(verb_t::requests, 
			  "%10s @ %12llu 0x%016llx: mem_hier_t::complete_request()\n", 
			  "mem_hier", external::get_current_cycle(), trans->phys_addr);

			  
	complete_stats(trans);
							  
	if (g_conf_handle_io_devices && trans->io_device && trans->write) {
		num_io_writes--;
	}
	
	if (trans->call_complete_request) {
        if (g_conf_mem_hier_response_tracking)
            tr_rec->delete_record(trans->cpu_id, trans);
        if (trans->random_trans) {
            ASSERT(g_conf_random_addresses);
            random_tester->complete_random_transaction(trans);
        } else {
		    mhh->complete_request((conf_object_t *) get_module_obj(), cpu, trans);
            if (g_conf_copy_transaction)
                delete_list.push_back(trans);
        }
	} else {
		ASSERT(!trans->may_stall || trans->is_prefetch());
		if (g_conf_copy_transaction)
			delete_list.push_back(trans);
		else
			delete trans;
	}
}

void
mem_hier_t::invalidate_address(conf_object_t *cpu, invalidate_addr_t *ia)
{
	VERBOSE_OUT(verb_t::requests, 
			  "%10s @ %12llu 0x%016llx: mem_hier_t::invalidate_address() size %u\n", 
			  "mem_hier", external::get_current_cycle(), ia->address,
			  ia->size);

	mhh->invalidate_address((conf_object_t *) get_module_obj(), cpu, ia);
}

mem_return_t
mem_hier_t::make_request(conf_object_t *cpu, mem_trans_t *trans)
{
	// Bootstrap mem-hier
	// TODO: fix
	if (!is_initialized){
		if (!config) read_runtime_config_file("");
		init();
	}
	
	ASSERT(trans->ini_ptr);
	ASSERT_WARN(trans->is_cacheable() || trans->is_io_space() || 
		trans->io_device);
	
	if (!trans->occurred(MEM_EVENT_DEFERRED)) {
		// Make a copy if we shouldn't call complete request and not deferred already
		if (!trans->random_trans && (g_conf_copy_transaction || !trans->call_complete_request)) {
			
			trans = new mem_trans_t(*trans);
		}

		ASSERT(trans->request_time == 0);
		trans->request_time = external::get_current_cycle();

		request_stats(cpu, trans);
	}
	
    if (g_conf_mem_hier_response_tracking && trans->call_complete_request)
        tr_rec->insert_record(trans->cpu_id, trans, g_cycles);
	
	mem_return_t ret;
	if (trans->io_device) {
		ret = handle_io_dev_request(cpu, trans);
	} else { 
		ret = handle_proc_request(cpu, trans);
        if (g_conf_random_addresses && !trans->ifetch && trans->may_stall
            && !trans->random_trans && (g_cycles % g_conf_random_transaction_frequency == 0)) {
            random_tester->generate_random_transactions(trans);
        }
	}

	switch (ret) {
	case MemComplete:
//        ASSERT(!trans->random_trans);
		trans->completed = true;
		complete_request(cpu, trans);
		break;
	
	case MemStall:
		defer_trans(trans);
		break;
	
	case MemMiss:
		// nothing
		break;
	
	default:
		FAIL;
	}
	return ret;
}

mem_return_t
mem_hier_t::handle_proc_request(conf_object_t *cpu, mem_trans_t *trans)
{
	ASSERT(trans->cpu_id < num_processors);
	// Assume first n-devices are Procs
	//   Multiplex request to appropriate processor
	generic_proc_t *proc = get_proc_object(cpu);
	
	VERBOSE_OUT(verb_t::requests, 
		"%10s @ %12llu 0x%016llx: mem_hier_t::make_request()\n", 
		"mem_hier", external::get_current_cycle(), trans->phys_addr);
		
	if (trans->dinst && trans->dinst->is_squashed()) {
		STAT_INC(stat_num_squash);
		return MemComplete;
	}

	if (!trans->may_stall) {
		if (!trans->atomic) {
			VERBOSE_OUT(verb_t::requests, 
				"%10s @ %12llu 0x%016llx: unstallable, nonatomic processor trans\n", 
				"mem_hier", external::get_current_cycle(), trans->phys_addr);
		}
		
		// Just complete when doing single-transaction atomics
		if (g_conf_single_trans_atomics) return MemComplete;	
	}
	
	if (!g_conf_cache_io_space && trans->is_io_space()) {
		return MemComplete;
	}
	
	// Stall new requests if pending io write
	if (g_conf_handle_io_devices && num_io_writes > 0) {
		// But don't stall the second in an atomic pair
		if (!trans->may_stall && !trans->atomic) { } 
		else return MemStall;
	}
	
	ASSERT(proc);
    return proc->make_request(cpu, trans);
}

mem_return_t
mem_hier_t::handle_io_dev_request(conf_object_t *cpu, mem_trans_t *trans)
{
	VERBOSE_OUT(verb_t::requests, 
		"%10s @ %12llu 0x%016llx: mem_hier_t::make_request() I/O %s\n", 
		"mem_hier", external::get_current_cycle(), trans->phys_addr,
		trans->read ? "read" : "write");
		
	if (!g_conf_handle_io_devices) return MemComplete;
	if (!g_conf_handle_iodev_reads && trans->read) return MemComplete;
	
	// Inc # outstanding IO writes (if we havn't done so already for trans)
	if (trans->write && !trans->occurred(MEM_EVENT_DEFERRED)) num_io_writes++;
	
	// io dev should be first device after all procs  FIXME: better way
	generic_io_dev_t *iodev = 
		static_cast<generic_io_dev_t*> (devices[num_processors]);
	
	return iodev->make_request(trans);
}

void
mem_hier_t::request_stats(conf_object_t *cpu, mem_trans_t *trans)
{
	// Stats
	STAT_INC(stat_num_requests);
	stat_request_size->inc_total(1, trans->size);
	if (trans->read) STAT_INC(stat_num_reads);
	if (trans->write) STAT_INC(stat_num_writes);
	if (trans->atomic) STAT_INC(stat_num_atomics);
	stat_cpu_histo->inc_total(1, trans->cpu_id);
	if (!trans->is_cacheable()) {
		STAT_INC(stat_num_uncache);
		if (trans->io_device) STAT_INC(stat_num_uncache_io_device);
		if (trans->is_io_space()) STAT_INC(stat_num_uncache_io_space);
	}
	if (trans->io_device) {
		STAT_INC(stat_num_io_device);
		if (!trans->is_io_space()) STAT_INC(stat_num_io_device_phys_space);
	}
	if (trans->is_io_space()) STAT_INC(stat_num_io_space);
	if (!trans->may_stall) STAT_INC(stat_num_may_not_stall);

	if (g_conf_watch_idle_loop && trans->ifetch) {
		if (trans->phys_addr <= (addr_t) g_conf_idle_max_pc &&
			trans->phys_addr >= (addr_t) g_conf_idle_min_pc)
		{
			if (!currently_idle[trans->cpu_id]) {
				currently_idle[trans->cpu_id] = true;
				idle_start[trans->cpu_id] = external::get_current_cycle();
			}
		}
		else {
			if (currently_idle[trans->cpu_id]) {
				currently_idle[trans->cpu_id] = false;
				stat_idle_cycles_histo->inc_total(
					external::get_current_cycle() - idle_start[trans->cpu_id],
					trans->cpu_id);
			}
		}
	}

	//stat_asi_histo->inc_total(1, trans->asi);
}


void
mem_hier_t::complete_stats(mem_trans_t *trans)
{
	if (trans->is_prefetch()) 
		return;
	
	stat_requesttime_histo->inc_total(1, external::get_current_cycle() - 
		trans->request_time);

	if (trans->occurred(MEM_EVENT_L1_HIT))
		STAT_INC(stat_l1_hit);
	if (trans->occurred(MEM_EVENT_L1_DEMAND_MISS))
		STAT_INC(stat_l1_miss);
	if (trans->occurred(MEM_EVENT_L1_COHER_MISS)) {
		STAT_INC(stat_l1_miss);
		STAT_INC(stat_l1_coher_miss);
	}
	if (trans->occurred(MEM_EVENT_L1_MSHR_PART_HIT))
		STAT_INC(stat_l1_mshr_part_hit);
	if (trans->occurred(MEM_EVENT_L1_STALL))
		STAT_INC(stat_l1_stall);

	if (trans->occurred(MEM_EVENT_L2_HIT))
		STAT_INC(stat_l2_hit);
	if (trans->occurred(MEM_EVENT_L2_DEMAND_MISS))
		STAT_INC(stat_l2_miss);
	if (trans->occurred(MEM_EVENT_L2_COHER_MISS)) {
		STAT_INC(stat_l2_miss);
		STAT_INC(stat_l2_coher_miss);
	}
	if (trans->occurred(MEM_EVENT_L2_MSHR_PART_HIT))
		STAT_INC(stat_l2_mshr_part_hit);
	if (trans->occurred(MEM_EVENT_L2_REQ_FWD))
		STAT_INC(stat_l2_req_fwd);
	if (trans->occurred(MEM_EVENT_L2_STALL))
		STAT_INC(stat_l2_stall);
}

void
mem_hier_t::defer_trans(mem_trans_t *trans)
{
	ASSERT(!trans->completed);

	trans->mark_event(MEM_EVENT_DEFERRED);

	// Does not have to be FIFO for SC if:
	//    - protocols all support speculative loads or processor sends only one
	//      load at a time
	//    - processor won't send more than one store at a time
	// TODO: impelment this per/processor or per/cache
	_stall_event_t *e = new _stall_event_t(trans, this, stall_delta,
		stall_event_handler);
	e->enqueue();
	num_deferred++;
}
	

bool
mem_hier_t::quiet_and_ready()
{
    return (is_quiet() && checkpoint_taken);
}


bool
mem_hier_t::is_quiet()
{
	if (num_deferred > 0) return false;

	// Check each device ...
	for (uint32 i = 0; i < num_devices; i++) {
		if (!devices[i]->is_quiet()) return false;
	}

	// ... and each link
	for (uint32 i = 0; i < num_links; i++) {
		if (!links[i]->is_quiet()) return false;
	}
	
	// Quiet!
	// Can have an outstanding print stats event
	ASSERT(external::get_num_events() == 0 || external::get_num_events() == 1);
	return true;
}


void
mem_hier_t::quiesce_event_handler(_quiesce_event_t *e)
{
	// TODO: fix checkpointing

	mem_hier_t *mem = e->get_context();
	
	if (mem->is_quiet()) {
		// We're good to go!
		//external::write_configuration(*filename);
        mem->write_checkpoint(g_conf_mem_hier_checkpoint_out);
		//external::quit(0);
	} else {
		VERBOSE_OUT(verb_t::checkpoints,
					"[%s] Mem-hier not quiet.  Will try again in %d cycles\n",
					mem->get_name(), 
					g_conf_quiesce_delta);

		// Otherwise wait another while
		e->enqueue();
	}
	
}

void
mem_hier_t::checkpoint_and_quit()
{
	if (g_conf_mem_hier_checkpoint_out == "" || g_conf_workload_checkpoint == "") {
		VERBOSE_OUT(verb_t::checkpoints, 
				  "[%s] ERROR: No condor checkpoint file specified!  Exiting\n",
				  mem_hier_t::ptr()->get_name());

	//	external::quit(0);
        return;
	}

    checkpoint_signal_pending = true;
	
	

	//	get_handler()->stall_processor();
	
	if (is_quiet()) {
        VERBOSE_OUT(verb_t::checkpoints,
			    "[%s] Received SIGTSTP.  Writing checkpoint %s\n",
				mem_hier_t::ptr()->get_name(), g_conf_mem_hier_checkpoint_out.c_str());

		// We're good to go!
		//external::write_configuration(g_conf_workload_checkpoint);
        write_checkpoint(g_conf_mem_hier_checkpoint_out);
		//external::quit(0);
	} else {
		VERBOSE_OUT(verb_t::checkpoints,
					"[%s] Mem-hier not quiet.  Will try again in %d cycles\n",
					mem_hier_t::ptr()->get_name(), 
					g_conf_quiesce_delta);

		_quiesce_event_t *e = new _quiesce_event_t(&g_conf_workload_checkpoint, this,
		                                           g_conf_quiesce_delta,
												   quiesce_event_handler);
		e->enqueue();
	}
}


void
mem_hier_t::stall_event_handler(_stall_event_t *e)
{
	mem_hier_t *mem = e->get_context();
	mem_trans_t *trans = e->get_data();
	delete e;
	mem->num_deferred--;
	
	ASSERT(!trans->completed);
	
	// TODO: fix:  should save cpu * somewhere else
	mem->make_request(trans->ini_ptr, trans);
}
	

void
mem_hier_t::clear_stats()
{
	stats->clear();
	for (uint32 i = 0; i < num_devices; i++) {
		devices[i]->clear_stats();
	}
}

void
mem_hier_t::print_stats()
{
    string fname = g_conf_stats_log;
    if (!fname.empty ())
		debugio_t::open_log (fname.c_str (), g_conf_overwrite_stats);
	// Flush out idle loop count
	if (g_conf_watch_idle_loop) {
		for (uint32 i = 0; i < num_processors; i++) {
			if (currently_idle[i]) {
				stat_idle_cycles_histo->inc_total(
					external::get_current_cycle() - idle_start[i], i);
			}
			idle_start[i] = external::get_current_cycle();
		}
	}
	
	stats->print();
	for (uint32 i = 0; i < num_devices; i++) {
		devices[i]->print_stats();
	}
    
    if (!fname.empty ()) 
		debugio_t::close_log ();
}
	
void
mem_hier_t::write_checkpoint(string filename)
{

	VERBOSE_OUT(verb_t::checkpoints, "mem_hier_t::write_checkpoint(): %s\n",
			  filename.c_str());

	FILE * file;
	file = fopen(filename.c_str(), "w");

	if (!file) {
		ERROR_OUT("Unable to open file %s\n!", filename.c_str());
		return;
	}
    
    // Output the counters
    fprintf(file, "%llu\n", g_cycles);
    for (uint32 i = 0; i < num_processors; i++)
        fprintf(file, "%llu ", initial_step_count[i]);
    fprintf(file, "\n");
    
    stats->to_file(file);
	
	for (uint32 i = 0; i < num_devices; i++) {
		devices[i]->to_file(file);
        devices[i]->stats_write_checkpoint(file);
        devices[i]->profiles_write_checkpoint(file);
	}
	
	fclose(file);

	VERBOSE_OUT(verb_t::checkpoints, "mem_hier_t::finished writing checkpoint\n");
	checkpoint_taken = true;
}

void
mem_hier_t::read_checkpoint(string filename)
{
	VERBOSE_OUT(verb_t::checkpoints, "mem_hier_t::read_checkpoint(): %s\n", filename.c_str());

	if (filename.length() == 0) {
		VERBOSE_OUT(verb_t::checkpoints, "None to read\n");
		return;
	}

	VERBOSE_OUT(verb_t::checkpoints, "reading...");
	
	FILE * file;
	file = fopen(filename.c_str(), "r");

    
    fscanf(file, "%llu\n", &g_cycles);
    for (uint32 i = 0; i < num_processors; i++)
        fscanf(file, "%llu ", &initial_step_count[i]);
    fscanf(file, "\n");
    
    
	stats->from_file(file);
    
	for (uint32 i = 0; i < num_devices; i++) {
		devices[i]->from_file(file);
        devices[i]->stats_read_checkpoint(file);
        devices[i]->profiles_read_checkpoint(file);
	}
	
	VERBOSE_OUT(verb_t::checkpoints, "done\n");
	
}
	

proc_object_t *
mem_hier_t::get_module_obj()
{
	return module_obj;
}

const char *
mem_hier_t::get_name()
{
	// TODO: fix
	return "mem-hier";
}
